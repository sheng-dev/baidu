{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "图像分类是根据图像的语义信息将不同类别图像区分开来，是计算机视觉中重要的基本问题\r\n",
    "\r\n",
    "猫狗分类属于图像分类中的粗粒度分类问题\r\n",
    "\r\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1cd9ef37036647c2afbbc866a7d2c14179f33cf1e2494d1f8f00de556d231452)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实践总体过程和步骤如下图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/b008c158886547649a9b06f6ae96df44447427fe65db4bac82b609334bd0d25c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**首先导入必要的包**\n",
    "\n",
    "paddle.fluid--->PaddlePaddle深度学习框架\n",
    "\n",
    "os------------->python的模块，可使用该模块对操作系统进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#导入需要的包\n",
    "\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step1:准备数据**\n",
    "**（1）数据集介绍**\n",
    "\n",
    "我们使用CIFAR10数据集。CIFAR10数据集包含60,000张32x32的彩色图片，10个类别，每个类包含6,000张。其中50,000张图片作为训练集，10000张作为验证集。这次我们只对其中的猫和狗两类进行预测。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/567016c028714d21bfe690dee70e9ea31512ba3575bd4d7caebbb7ade05c72ac)\n",
    "\n",
    "**(2)train_reader和test_reader**\n",
    "\n",
    "自定义读取器处理训练集和测试集\n",
    "\n",
    "paddle.reader.shuffle()表示每次缓存BUF_SIZE个数据项，并进行打乱\n",
    "\n",
    "paddle.batch()表示每BATCH_SIZE组成一个batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tar -zxvf /home/aistudio/data/data9154/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\r\n",
    "    import pickle\r\n",
    "    with open(file, 'rb') as fo:\r\n",
    "        dict = pickle.load(fo, encoding='bytes')\r\n",
    "    return dict\r\n",
    "\r\n",
    "print(unpickle(\"cifar-10-batches-py/data_batch_1\").keys())\r\n",
    "print(unpickle(\"cifar-10-batches-py/test_batch\").keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_mapper(sample):\r\n",
    "    img, label = sample\r\n",
    "    #将img数组进行进行归一化处理，得到0到1之间的数值\r\n",
    "    img= img.flatten().astype('float32')/255.0\r\n",
    "    return img, label\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_mapper(sample):\r\n",
    "    img, label = sample\r\n",
    "    #将img数组进行进行归一化处理，得到0到1之间的数值\r\n",
    "    img= img.flatten().astype('float32')/255.0\r\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 对自定义数据集创建训练集train的reader\r\n",
    "def train_r( buffered_size=1024):\r\n",
    "    def reader():\r\n",
    "        train_dict={}\r\n",
    "        train_dict1=unpickle(\"cifar-10-batches-py/data_batch_1\")\r\n",
    "        train_dict.update(train_dict1)\r\n",
    "        train_dict2=unpickle(\"cifar-10-batches-py/data_batch_2\")\r\n",
    "        train_dict.update(train_dict2)\r\n",
    "        train_dict3=unpickle(\"cifar-10-batches-py/data_batch_3\")\r\n",
    "        train_dict.update(train_dict3)\r\n",
    "        train_dict4=unpickle(\"cifar-10-batches-py/data_batch_4\")\r\n",
    "        train_dict.update(train_dict4)\r\n",
    "        train_dict5=unpickle(\"cifar-10-batches-py/data_batch_5\")\r\n",
    "        train_dict.update(train_dict5)\r\n",
    "        X=train_dict[b'data']\r\n",
    "        Y=train_dict[b'labels']\r\n",
    "        for (x,y) in zip(X,Y):  \r\n",
    "            yield x, int(y) \r\n",
    "    return paddle.reader.xmap_readers(train_mapper, reader,cpu_count(), buffered_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 对自定义数据集创建训练集train的reader\r\n",
    "\r\n",
    "def test_r( buffered_size=1024):\r\n",
    "    def reader():\r\n",
    "        test_dict=unpickle(\"cifar-10-batches-py/test_batch\")\r\n",
    "        X=test_dict[b'data']\r\n",
    "        Y=test_dict[b'labels']\r\n",
    "        for (x,y) in zip(X,Y):  \r\n",
    "            yield x, int(y) \r\n",
    "    return paddle.reader.xmap_readers(test_mapper, reader,cpu_count(), buffered_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "#用于训练的数据提供器\n",
    "train_reader = train_r()\n",
    "train_reader = paddle.batch(\n",
    "    paddle.reader.shuffle(\n",
    "        reader=train_reader,buf_size=128*100),\n",
    "    batch_size=BATCH_SIZE)\n",
    "#用于测试的数据提供器\n",
    "test_reader = test_r()\n",
    "test_reader = paddle.batch(\n",
    "    paddle.reader.shuffle(\n",
    "        reader=test_reader,buf_size=300),\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step2.网络配置**\n",
    "\n",
    "**（1）网络搭建**\n",
    "\n",
    "*** **CNN网络模型**\n",
    "\n",
    "在CNN模型中，卷积神经网络能够更好的利用图像的结构信息。下面定义了一个较简单的卷积神经网络。显示了其结构：输入的二维图像，先经过三次卷积层、池化层和Batchnorm，再经过全连接层，最后使用softmax分类作为输出层。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/98b9b702cce040fb8a874e28eae6d34ace6025a2c9444cdd954ab5f14d69cfdc)\n",
    "\n",
    "**池化**是非线性下采样的一种形式，主要作用是通过减少网络的参数来减小计算量，并且能够在一定程度上控制过拟合。通常在卷积层的后面会加上一个池化层。paddlepaddle池化默认为最大池化。是用不重叠的矩形框将输入层分成不同的区域，对于每个矩形框的数取最大值作为输出\n",
    "\n",
    "**Batchnorm**顾名思义是对每batch个数据同时做一个norm。作用就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convolutional_neural_network(img):\n",
    "    # 第一个卷积-池化层\n",
    "    conv_pool_1 = fluid.nets.simple_img_conv_pool(\n",
    "        input=img,         # 输入图像\n",
    "        filter_size=5,     # 滤波器的大小\n",
    "        num_filters=20,    # filter 的数量。它与输出的通道相同\n",
    "        pool_size=2,       # 池化核大小2*2\n",
    "        pool_stride=2,     # 池化步长\n",
    "        act=\"relu\")        # 激活类型\n",
    "    conv_pool_1 = fluid.layers.batch_norm(conv_pool_1)\n",
    "    # 第二个卷积-池化层\n",
    "    conv_pool_2 = fluid.nets.simple_img_conv_pool(\n",
    "        input=conv_pool_1,\n",
    "        filter_size=5,\n",
    "        num_filters=50,\n",
    "        pool_size=2,\n",
    "        pool_stride=2,\n",
    "        act=\"relu\")\n",
    "    conv_pool_2 = fluid.layers.batch_norm(conv_pool_2)\n",
    "    # 第三个卷积-池化层\n",
    "    conv_pool_3 = fluid.nets.simple_img_conv_pool(\n",
    "        input=conv_pool_2,\n",
    "        filter_size=5,\n",
    "        num_filters=50,\n",
    "        pool_size=2,\n",
    "        pool_stride=2,\n",
    "        act=\"relu\")\n",
    "    # 以softmax为激活函数的全连接输出层，10类数据输出10个数字\n",
    "    prediction = fluid.layers.fc(input=conv_pool_3, size=10, act='softmax')\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（2）定义数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义输入数据\n",
    "data_shape = [3, 32, 32]\n",
    "images = fluid.layers.data(name='images', shape=data_shape, dtype='float32')\n",
    "label = fluid.layers.data(name='label', shape=[1], dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（3）获取分类器**\n",
    "\n",
    "下面cell里面使用了CNN方式进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取分类器，用cnn进行分类\n",
    "predict =  convolutional_neural_network(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（4）定义损失函数和准确率**\n",
    "\n",
    "这次使用的是交叉熵损失函数，该函数在分类任务上比较常用。\n",
    "\n",
    "定义了一个损失函数之后，还有对它求平均值，因为定义的是一个Batch的损失值。\n",
    "\n",
    "同时我们还可以定义一个准确率函数，这个可以在我们训练的时候输出分类的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取损失函数和准确率\n",
    "cost = fluid.layers.cross_entropy(input=predict, label=label) # 交叉熵\n",
    "avg_cost = fluid.layers.mean(cost)                            # 计算cost中所有元素的平均值\n",
    "acc = fluid.layers.accuracy(input=predict, label=label)       #使用输入和标签计算准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（5）定义优化方法**\n",
    "\n",
    "这次我们使用的是Adam优化方法，同时指定学习率为0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取测试程序\n",
    "test_program = fluid.default_main_program().clone(for_test=True)\n",
    "# 定义优化方法\n",
    "optimizer =fluid.optimizer.Adam(learning_rate=0.001)\n",
    "optimizer.minimize(avg_cost)\n",
    "print(\"完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述模型配置完毕后，得到两个fluid.Program：fluid.default_startup_program() 与fluid.default_main_program() 配置完毕了。\n",
    "\n",
    "参数初始化操作会被写入fluid.default_startup_program()\n",
    "\n",
    "fluid.default_main_program()用于获取默认或全局main program(主程序)。该主程序用于训练和测试模型。fluid.layers 中的所有layer函数可以向 default_main_program 中添加算子和变量。default_main_program 是fluid的许多编程接口（API）的Program参数的缺省值。例如,当用户program没有传入的时候， Executor.run() 会默认执行 default_main_program 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step3.模型训练 and Step4.模型评估**\n",
    "\n",
    "**（1）创建Executor**\n",
    "\n",
    "首先定义运算场所 fluid.CPUPlace()和 fluid.CUDAPlace(0)分别表示运算场所为CPU和GPU\n",
    "\n",
    "Executor:接收传入的program，通过run()方法运行program。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义使用CPU还是GPU，使用CPU时use_cuda = False,使用GPU时use_cuda = True\n",
    "use_cuda = False\n",
    "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
    "\n",
    "exe = fluid.Executor(place)\n",
    "exe.run(fluid.default_startup_program())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)定义数据映射器**\n",
    "\n",
    "DataFeeder 负责将reader(读取器)返回的数据转成一种特殊的数据结构，使它们可以输入到 Executor\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feeder = fluid.DataFeeder( feed_list=[images, label],place=place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（3）定义绘制训练过程的损失值和准确率变化趋势的方法draw_train_process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_iter=0\r\n",
    "all_train_iters=[]\r\n",
    "all_train_costs=[]\r\n",
    "all_train_accs=[]\r\n",
    "\r\n",
    "def draw_train_process(title,iters,costs,accs,label_cost,lable_acc):\r\n",
    "    plt.title(title, fontsize=24)\r\n",
    "    plt.xlabel(\"iter\", fontsize=20)\r\n",
    "    plt.ylabel(\"cost/acc\", fontsize=20)\r\n",
    "    plt.plot(iters, costs,color='red',label=label_cost) \r\n",
    "    plt.plot(iters, accs,color='green',label=lable_acc) \r\n",
    "    plt.legend()\r\n",
    "    plt.grid()\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（3）训练并保存模型**\n",
    "\n",
    "Executor接收传入的program,并根据feed map(输入映射表)和fetch_list(结果获取表) 向program中添加feed operators(数据输入算子)和fetch operators（结果获取算子)。 feed map为该program提供输入数据。fetch_list提供program训练结束后用户预期的变量。\n",
    "\n",
    "每一个Pass训练结束之后，再使用验证集进行验证，并打印出相应的损失值cost和准确率acc。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCH_NUM = 2\n",
    "model_save_dir = \"/home/aistudio/work/catdog.inference.model\"\n",
    "\n",
    "for pass_id in range(EPOCH_NUM):\n",
    "    # 开始训练\n",
    "    for batch_id, data in enumerate(train_reader()):                        #遍历train_reader的迭代器，并为数据加上索引batch_id\n",
    "        train_cost,train_acc = exe.run(program=fluid.default_main_program(),#运行主程序\n",
    "                             feed=feeder.feed(data),                        #喂入一个batch的数据\n",
    "                             fetch_list=[avg_cost, acc])                    #fetch均方误差和准确率\n",
    "\n",
    "        \n",
    "        all_train_iter=all_train_iter+BATCH_SIZE\n",
    "        all_train_iters.append(all_train_iter)\n",
    "        all_train_costs.append(train_cost[0])\n",
    "        all_train_accs.append(train_acc[0])\n",
    "        \n",
    "        #每100次batch打印一次训练、进行一次测试\n",
    "        if batch_id % 100 == 0:                                             \n",
    "            print('Pass:%d, Batch:%d, Cost:%0.5f, Accuracy:%0.5f' % \n",
    "            (pass_id, batch_id, train_cost[0], train_acc[0]))\n",
    "            \n",
    "\n",
    "    # 开始测试\n",
    "    test_costs = []                                                         #测试的损失值\n",
    "    test_accs = []                                                          #测试的准确率\n",
    "    for batch_id, data in enumerate(test_reader()):\n",
    "        test_cost, test_acc = exe.run(program=test_program,                 #执行训练程序\n",
    "                                      feed=feeder.feed(data),               #喂入数据\n",
    "                                      fetch_list=[avg_cost, acc])           #fetch 误差、准确率\n",
    "        test_costs.append(test_cost[0])                                     #记录每个batch的误差\n",
    "        test_accs.append(test_acc[0])                                       #记录每个batch的准确率\n",
    "    \n",
    "    # 求测试结果的平均值\n",
    "    test_cost = (sum(test_costs) / len(test_costs))                         #计算误差平均值（误差和/误差的个数）\n",
    "    test_acc = (sum(test_accs) / len(test_accs))                            #计算准确率平均值（ 准确率的和/准确率的个数）\n",
    "    print('Test:%d, Cost:%0.5f, ACC:%0.5f' % (pass_id, test_cost, test_acc))\n",
    "    \n",
    "#保存模型\n",
    "# 如果保存路径不存在就创建\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "print ('save models to %s' % (model_save_dir))\n",
    "fluid.io.save_inference_model(model_save_dir,\n",
    "                              ['images'],\n",
    "                              [predict],\n",
    "                              exe)\n",
    "print('训练模型保存完成！')\n",
    "draw_train_process(\"training\",all_train_iters,all_train_costs,all_train_accs,\"trainning cost\",\"trainning acc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step5.模型预测**\n",
    "\n",
    "**（1）创建预测用的Executor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infer_exe = fluid.Executor(place)\n",
    "inference_scope = fluid.core.Scope() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)图片预处理**\n",
    "\n",
    "在预测之前，要对图像进行预处理。\n",
    "\n",
    "首先将图片大小调整为32*32，接着将图像转换成一维向量，最后再对一维向量进行归一化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_image(file):\n",
    "        #打开图片\n",
    "        im = Image.open(file)\n",
    "        #将图片调整为跟训练数据一样的大小  32*32，                   设定ANTIALIAS，即抗锯齿.resize是缩放\n",
    "        im = im.resize((32, 32), Image.ANTIALIAS)\n",
    "        #建立图片矩阵 类型为float32\n",
    "        im = np.array(im).astype(np.float32)\n",
    "        #矩阵转置 \n",
    "        im = im.transpose((2, 0, 1))                               \n",
    "        #将像素值从【0-255】转换为【0-1】\n",
    "        im = im / 255.0\n",
    "        #print(im)       \n",
    "        im = np.expand_dims(im, axis=0)\n",
    "        # 保持和之前输入image维度一致\n",
    "        print('im_shape的维度：',im.shape)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)开始预测**\r\n",
    "\r\n",
    "通过fluid.io.load_inference_model，预测器会从params_dirname中读取已经训练好的模型，来对从未遇见过的数据进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with fluid.scope_guard(inference_scope):\n",
    "    #从指定目录中加载 推理model(inference model)\n",
    "    [inference_program, # 预测用的program\n",
    "     feed_target_names, # 是一个str列表，它包含需要在推理 Program 中提供数据的变量的名称。 \n",
    "     fetch_targets] = fluid.io.load_inference_model(model_save_dir,#fetch_targets：是一个 Variable 列表，从中我们可以得到推断结果。\n",
    "                                                    infer_exe)     #infer_exe: 运行 inference model的 executor\n",
    "    \n",
    "    infer_path='/home/aistudio/data/data1909/dog.png'\n",
    "    img = Image.open(infer_path)\n",
    "    plt.imshow(img)   \n",
    "    plt.show()    \n",
    "    \n",
    "    img = load_image(infer_path)\n",
    "\n",
    "    results = infer_exe.run(inference_program,                 #运行预测程序\n",
    "                            feed={feed_target_names[0]: img},  #喂入要预测的img\n",
    "                            fetch_list=fetch_targets)          #得到推测结果\n",
    "    print('results',results)\n",
    "    label_list = [\n",
    "        \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\",\n",
    "        \"ship\", \"truck\"\n",
    "        ]\n",
    "    print(\"infer results: %s\" % label_list[np.argmax(results[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.4.1 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
